{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "477fc4e8-3941-4f8a-9eb2-a7716977a6cb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any, Optional\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "class DataSink:\n",
    "    \"\"\"\n",
    "    Abstract base class for data sinks. This class defines the common structure \n",
    "    for any data sink that will load a DataFrame to a specified location (path) \n",
    "    using a particular method (e.g., 'overwrite', 'append').\n",
    "    Subclasses must implement the `load_data_frame` method.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path: str, method: str, params: Dict[str, Any], df: DataFrame):\n",
    "        \"\"\"\n",
    "        Initializes the DataSink class with the required attributes.\n",
    "        \n",
    "        Parameters:\n",
    "        path (str): The file path or destination where the data will be loaded.\n",
    "        method (str): The method used to load the data, e.g., 'overwrite' or 'append'.\n",
    "        params (dict): Additional parameters that may be required for data loading.\n",
    "        df (DataFrame): The DataFrame to be loaded.\n",
    "        \"\"\"\n",
    "        self.path = path          # The destination path for the DataFrame\n",
    "        self.method = method      # The method for loading the data (e.g., 'overwrite')\n",
    "        self.params = params      # Additional parameters for loading\n",
    "        self.df = df              # The DataFrame to be loaded\n",
    "\n",
    "    def load_data_frame(self) -> None:\n",
    "        \"\"\"\n",
    "        Abstract method that should be implemented by subclasses. \n",
    "        This method will handle the logic for loading the DataFrame to the sink.\n",
    "        \n",
    "        Raises:\n",
    "        ValueError: If the method is not defined in a subclass.\n",
    "        \"\"\"\n",
    "        raise ValueError(\"Method not defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "446264d8-77c0-4262-995a-a589dfb82de5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class LoadToADLSGEN2WithPartition(DataSink):\n",
    "    \"\"\"\n",
    "    A concrete implementation of the DataSink class for loading data into \n",
    "    Azure Data Lake Storage Gen2 (ADLS Gen2) with partitioning.\n",
    "    This class partitions the data based on specified columns before saving it.\n",
    "    \"\"\"\n",
    "\n",
    "    def load_data_frame(self) -> None:\n",
    "        \"\"\"\n",
    "        Loads the DataFrame into ADLS Gen2 with partitioning. \n",
    "        It retrieves the partition columns from the parameters, then writes \n",
    "        the data to the specified path using the given write mode.\n",
    "        \n",
    "        Parameters:\n",
    "        None\n",
    "        \n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        # Get the partition columns from the parameters dictionary\n",
    "        partitionByColumnsList: List[str] = self.params.get(\"partitionByColumns\")\n",
    "        \n",
    "        # Write the DataFrame with partitioning and save to the specified path\n",
    "        self.df.write.format(\"parquet\").mode(self.method).partitionBy(*partitionByColumnsList).save(self.path)\n",
    "\n",
    "\n",
    "class LoadToADLSGEN2(DataSink):\n",
    "    \"\"\"\n",
    "    A concrete implementation of the DataSink class for loading data into \n",
    "    Azure Data Lake Storage Gen2 (ADLS Gen2) without partitioning.\n",
    "    This class simply saves the data without any partitioning.\n",
    "    \"\"\"\n",
    "\n",
    "    def load_data_frame(self) -> None:\n",
    "        \"\"\"\n",
    "        Loads the DataFrame into ADLS Gen2 without partitioning. \n",
    "        It writes the data to the specified path using the given write mode.\n",
    "        \n",
    "        Parameters:\n",
    "        None\n",
    "        \n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        # Write the DataFrame and save to the specified path without partitioning\n",
    "        self.df.write.format(\"parquet\").mode(self.method).save(self.path)\n",
    "\n",
    "\n",
    "class LoadToADLSGEN2WithDelta(DataSink):\n",
    "    \"\"\"\n",
    "    A concrete implementation of the DataSink class for loading data into \n",
    "    Azure Data Lake Storage Gen2 (ADLS Gen2) using the Delta format.\n",
    "    This class saves the data in Delta format and allows schema overwriting.\n",
    "    \"\"\"\n",
    "\n",
    "    def load_data_frame(self) -> None:\n",
    "        \"\"\"\n",
    "        Loads the DataFrame into ADLS Gen2 in Delta format. \n",
    "        It writes the data with schema overwriting enabled and saves it using \n",
    "        the Delta format to the specified path.\n",
    "        \n",
    "        Parameters:\n",
    "        None\n",
    "        \n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        # Write the DataFrame in Delta format with schema overwriting, and save to the specified path\n",
    "        self.df.write.format(\"delta\").mode(self.method).option(\"overwriteSchema\", True).save(self.path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a7183c5-2b26-4651-9851-25eb35503f1d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_data_sink(sink_type: str, df: DataFrame, path: str, method: str, params: Optional[dict] = None) -> DataSink:\n",
    "    \"\"\"\n",
    "    Factory function to return the appropriate DataSink object based on the sink type.\n",
    "    \n",
    "    Parameters:\n",
    "    sink_type (str): The type of data sink to create. Valid options are 'adls', \n",
    "                     'adls_with_partition', and 'delta'.\n",
    "    df (DataFrame): The DataFrame to be loaded.\n",
    "    path (str): The file path where the DataFrame will be saved.\n",
    "    method (str): The write mode to use (e.g., 'overwrite', 'append').\n",
    "    params (dict, optional): Additional parameters for the DataSink (e.g., partition columns). \n",
    "                             Defaults to None.\n",
    "    \n",
    "    Returns:\n",
    "    DataSink: A subclass of `DataSink` that implements the `load_data_frame` method.\n",
    "    \n",
    "    Raises:\n",
    "    ValueError: If an unsupported `sink_type` is provided.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Return an ADLS Gen2 loader without partitioning\n",
    "    if sink_type == \"adls\":\n",
    "        return LoadToADLSGEN2(path, method, params, df)\n",
    "    \n",
    "    # Return an ADLS Gen2 loader with partitioning\n",
    "    elif sink_type == \"adls_with_partition\":\n",
    "        return LoadToADLSGEN2WithPartition(path, method, params, df)\n",
    "    \n",
    "    # Return a Delta format loader for ADLS Gen2\n",
    "    elif sink_type == \"delta\":\n",
    "        return LoadToADLSGEN2WithDelta(path, method, params, df)\n",
    "    \n",
    "    # Raise an error for unsupported sink types\n",
    "    else:\n",
    "        raise ValueError(f\"Not implemented for this {sink_type} type\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Loader_Factory",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
