{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8d3054c-b215-474e-a326-9598c97226ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nThis script allows for different transformations while conforming to the factory pattern\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "This script allows for different transformations while conforming to the factory pattern\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8deb7ad9-3aef-4d58-9cfd-c5889698b7a7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"./Extractor\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "234a4a4c-1ac6-412d-9670-f0fbb62316e5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "import pyspark.sql.functions as sf\n",
    "from pyspark.sql.window import Window\n",
    "from typing import Dict\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0bfc8e3-76e4-40b0-8f6b-980f5d926a31",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class Transform:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def transform(self,input_df):\n",
    "        \"\"\"Abstract class method needs to be defined in all child classes\"\"\"\n",
    "        raise ValueError(\"Method not defined\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79a6c9f0-4a94-4794-88ee-833525c3ebc0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Bronze to Silver Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e55a2f7e-94c5-4c83-8c9a-26799f5e97fc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class CustTableTransformations(Transform):\n",
    "    \"\"\"\n",
    "    This class provides transformations on the 'customer' table. It extends \n",
    "    the base Transform class and applies transformations such as concatenating \n",
    "    first and last names into a full name and handling null values in the 'active' column.\n",
    "    \"\"\"\n",
    "\n",
    "    def transform(self, inputDFs: Dict[str, DataFrame]) -> Dict[str, DataFrame]:\n",
    "        \"\"\"\n",
    "        Transforms the input DataFrame for 'customer' by:\n",
    "        - Creating a 'full_name' column by concatenating the 'first_name' and 'last_name' columns.\n",
    "        - Dropping the 'first_name' and 'last_name' columns from the DataFrame.\n",
    "        - Handling null values in the 'active' column by replacing them with 0.\n",
    "\n",
    "        Parameters:\n",
    "        inputDFs (dict): Dictionary of input DataFrames. Expected key:\n",
    "                         - \"customer\": DataFrame containing customer data.\n",
    "                         \n",
    "        Returns:\n",
    "        dict: The updated inputDFs dictionary with transformations applied to the \n",
    "              'customer' DataFrame.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Ensure all values in inputDFs are Spark DataFrames\n",
    "            for df in inputDFs.values():\n",
    "                assert isinstance(df, DataFrame), \"Non-DataFrame value found in input dictionary.\"\n",
    "\n",
    "            # Extract the 'customer' DataFrame from the input dictionary\n",
    "            customer_df = inputDFs[\"customer\"]\n",
    "\n",
    "            # Create a new 'full_name' column by concatenating 'first_name' and 'last_name'\n",
    "            # and then drop the 'first_name' and 'last_name' columns\n",
    "            customer_df = customer_df.withColumn(\"full_name\", \n",
    "                                             sf.concat(sf.col(\"first_name\"), \n",
    "                                                       sf.lit(\" \"), \n",
    "                                                       sf.col(\"last_name\"))) \\\n",
    "                                 .drop(\"first_name\", \"last_name\")\n",
    "\n",
    "            # Handle null values in the 'active' column by replacing them with 0\n",
    "            customer_df = customer_df.withColumn(\"active\", \n",
    "                                             sf.when(sf.col(\"active\").isNull(), \n",
    "                                                     sf.lit(0)).otherwise(sf.col(\"active\")))\n",
    "\n",
    "            # Update the input dictionary with the transformed 'customer' DataFrame\n",
    "            inputDFs[\"customer\"] = customer_df\n",
    "\n",
    "            # Return the updated input DataFrame dictionary\n",
    "            return inputDFs\n",
    "        except AssertionError as e:\n",
    "            print(\"Error: All values in the input dictionary must be Spark DataFrames.\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3166caa3-bfb8-467c-9187-c52249a6a725",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-177671288233407>, line 1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mPaymentsTableUnionTransform\u001B[39;00m(Transform):\n",
       "\u001B[1;32m      2\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m      3\u001B[0m \u001B[38;5;124;03m    A class that transforms a dictionary of Spark DataFrames by unifying all DataFrames\u001B[39;00m\n",
       "\u001B[1;32m      4\u001B[0m \u001B[38;5;124;03m    whose names contain 'payment_' and storing the result in the 'payment' key of the output dictionary.\u001B[39;00m\n",
       "\u001B[1;32m      5\u001B[0m \u001B[38;5;124;03m    The dictionary keys that have 'payment_' are actually partitions of a single table. Hence, we use union to\u001B[39;00m\n",
       "\u001B[1;32m      6\u001B[0m \u001B[38;5;124;03m    create dataframe containing the full data.\u001B[39;00m\n",
       "\u001B[1;32m      7\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
       "\u001B[1;32m      9\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtransform\u001B[39m(\u001B[38;5;28mself\u001B[39m, inputDFs: Dict[\u001B[38;5;28mstr\u001B[39m, DataFrame]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Dict[\u001B[38;5;28mstr\u001B[39m, DataFrame]:\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'Transform' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "NameError",
        "evalue": "name 'Transform' is not defined"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'Transform' is not defined"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
        "File \u001B[0;32m<command-177671288233407>, line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mPaymentsTableUnionTransform\u001B[39;00m(Transform):\n\u001B[1;32m      2\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;124;03m    A class that transforms a dictionary of Spark DataFrames by unifying all DataFrames\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;124;03m    whose names contain 'payment_' and storing the result in the 'payment' key of the output dictionary.\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;124;03m    The dictionary keys that have 'payment_' are actually partitions of a single table. Hence, we use union to\u001B[39;00m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;124;03m    create dataframe containing the full data.\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m      9\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtransform\u001B[39m(\u001B[38;5;28mself\u001B[39m, inputDFs: Dict[\u001B[38;5;28mstr\u001B[39m, DataFrame]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Dict[\u001B[38;5;28mstr\u001B[39m, DataFrame]:\n",
        "\u001B[0;31mNameError\u001B[0m: name 'Transform' is not defined"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "class PaymentsTableUnionTransform(Transform):\n",
    "    \"\"\"\n",
    "    A class that transforms a dictionary of Spark DataFrames by unifying all DataFrames\n",
    "    whose names contain 'payment_' and storing the result in the 'payment' key of the output dictionary.\n",
    "    The dictionary keys that have 'payment_' are actually partitions of a single table. Hence, we use union to\n",
    "    create dataframe containing the full data.\n",
    "    \"\"\"\n",
    "\n",
    "    def transform(self, inputDFs: Dict[str, DataFrame]) -> Dict[str, DataFrame]:\n",
    "        \"\"\"\n",
    "        Transforms the input DataFrames by performing a union operation on all DataFrames\n",
    "        whose names contain 'payment_' and adding the resulting unified DataFrame \n",
    "        back into the dictionary under the key 'payment'.\n",
    "\n",
    "        Args:\n",
    "            inputDFs (Dict[str, DataFrame]): Dictionary of Spark DataFrames with table names as keys.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, DataFrame]: A transformed dictionary where keys are table names and values are DataFrames.\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            # Ensure all values in inputDFs are Spark DataFrames\n",
    "            for df in inputDFs.values():\n",
    "                assert isinstance(df, DataFrame), \"Non-DataFrame value found in input dictionary.\"\n",
    "\n",
    "            # Filter and union DataFrames whose names contain \"payment_\"\n",
    "            union_dfs = [df for name, df in inputDFs.items() if \"payment_\" in name]\n",
    "            \n",
    "            if union_dfs:\n",
    "                # Perform the union operation if there are DataFrames to union\n",
    "                union_df = reduce(lambda a, b: a.union(b), union_dfs)\n",
    "\n",
    "                # Add the unified DataFrame to the dictionary under the 'payment' key\n",
    "                inputDFs[\"payment\"] = union_df\n",
    "\n",
    "            # Remove individual 'payment_' DataFrames, keeping the unified DataFrame\n",
    "            return {name: df for name, df in inputDFs.items() if \"payment_\" not in name or name == \"payment\"}\n",
    "\n",
    "        except AssertionError as e:\n",
    "            print(\"Error: All values in the input dictionary must be Spark DataFrames.\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred: {e}\")\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd6d891e-02a2-4fbc-aa2a-5cbf66520ca1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "Silver To Gold Layer Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fad7461b-9f12-412f-9c7c-770257898743",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class Top10PercentCustomerTransformation(Transform):\n",
    "    \"\"\"\n",
    "    A transformation class that calculates the top 10% of customers based on total sales and rental frequency.\n",
    "\n",
    "    Inherits from the Transform class and performs the following steps:\n",
    "    - Joins customer data with payment and rental data.\n",
    "    - Aggregates sales and rental frequency by customer.\n",
    "    - Selects the top 10% customers based on total sales and rental frequency.\n",
    "    \"\"\"\n",
    "\n",
    "    def transform(self, inputDFs: Dict[str, DataFrame]) -> Dict[str, DataFrame]:\n",
    "        \"\"\"\n",
    "        Transforms the input DataFrames to calculate the top 10% customers by total sales and rental frequency.\n",
    "\n",
    "        Args:\n",
    "            inputDFs (Dict[str, DataFrame]): A dictionary containing the input DataFrames with keys:\n",
    "                                             'customer', 'film', 'rental', 'inventory', 'payment'.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, DataFrame]: A dictionary with two DataFrames:\n",
    "                                  'top_customers_by_sales' for the top 10% customers by sales,\n",
    "                                  'top_customers_by_frequency' for the top 10% customers by rental frequency.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Ensure all values in inputDFs are Spark DataFrames\n",
    "            for df in inputDFs.values():\n",
    "                assert isinstance(df, DataFrame), \"Non-DataFrame value found in input dictionary.\"\n",
    "            \n",
    "            # Extract individual DataFrames from the input dictionary\n",
    "            customer_df = inputDFs[\"customer\"]\n",
    "            payment_df = inputDFs[\"payment\"]\n",
    "            rental_df = inputDFs[\"rental\"]\n",
    "\n",
    "\n",
    "            # Join customer_df with payment_df and retain relevant columns for sales aggregation\n",
    "            joined_df1 = customer_df.join(payment_df, customer_df[\"customer_id\"] == payment_df[\"customer_id\"], \"inner\")\\\n",
    "            .drop(payment_df[\"customer_id\"])\\\n",
    "            .filter(sf.col(\"activebool\") == True)\n",
    "        \n",
    "            # Join customer_df with rental_df and retain relevant columns for rental frequency aggregation\n",
    "            joined_df2 = customer_df.join(rental_df, customer_df[\"customer_id\"] == rental_df[\"customer_id\"], \"inner\").drop(rental_df[\"customer_id\"])\\\n",
    "            .filter(sf.col(\"activebool\") == True)\n",
    "       \n",
    "            # Aggregate the sales data by customer_id\n",
    "            top_cust_by_sales_df = (\n",
    "                joined_df1\n",
    "                .groupBy(sf.col(\"customer_id\"))\n",
    "                .agg(sf.sum(sf.col(\"amount\").cast(\"double\")).alias(\"Total_Spent\"))\n",
    "            )\n",
    "       \n",
    "            # Aggregate the rental data by customer_id\n",
    "            top_cust_by_freq_df = (\n",
    "                joined_df2\n",
    "                .groupBy(sf.col(\"customer_id\"))\n",
    "                .agg((sf.count(sf.col(\"rental_id\")).alias(\"Total_Frequency\")))\n",
    "            )\n",
    "\n",
    "            # Calculate the number of rows for the top 10% customers\n",
    "            limit_value_sales = int(top_cust_by_sales_df.count() * 0.1)\n",
    "            limit_value_freq = int(top_cust_by_freq_df.count() * 0.1)\n",
    "       \n",
    "            # Join the aggregated sales data back with the customer_df to get first_name and last_name\n",
    "            final_result_cust_sale_df = top_cust_by_sales_df.join(customer_df, \"customer_id\")\\\n",
    "                .select(\"customer_id\", \"full_name\", \n",
    "                        sf.round(sf.col(\"Total_Spent\").cast(\"double\"), 2).alias(\"Total_Spent\"))\\\n",
    "                .limit(limit_value_sales)\\\n",
    "                .orderBy(\"Total_Spent\", ascending=False)\n",
    "        \n",
    "            # Join the aggregated rental frequency data back with the customer_df to get first_name and last_name\n",
    "            final_result_cust_freq_df = top_cust_by_freq_df.join(customer_df, \"customer_id\")\\\n",
    "                .select(\"customer_id\", \"full_name\", \"Total_Frequency\")\\\n",
    "                .limit(limit_value_freq)\\\n",
    "                .orderBy(\"Total_Frequency\", ascending=False)\n",
    "\n",
    "            # Return the results as a dictionary of DataFrames\n",
    "            return {\n",
    "                \"top_customers_by_sales_df\": final_result_cust_sale_df, \n",
    "                \"top_customers_by_frequency_df\": final_result_cust_freq_df\n",
    "            }\n",
    "        except AssertionError as e:\n",
    "            print(\"Error: All values in the input dictionary must be Spark DataFrames.\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred: {e}\")\n",
    "            traceback.print_exc()  # This will print the full stack trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bc4997c-79e0-457b-9db3-8c4b6ef9ec9d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class SalesRevenueByGeographyTransformation(Transform):\n",
    "    \"\"\"\n",
    "    This class provides transformation logic to calculate sales revenue \n",
    "    by geography, specifically by district and city. It extends the base \n",
    "    Transform class and implements the required transformation logic on input DataFrames.\n",
    "    \"\"\"\n",
    "\n",
    "    def transform(self, inputDFs: Dict[str, DataFrame]) -> Dict[str, DataFrame]:\n",
    "        \"\"\"\n",
    "        Transforms the input DataFrames to calculate total sales by district and city.\n",
    "        The DataFrames include store, address, city, customer, and payment data, which \n",
    "        are joined and filtered based on active status before aggregating sales.\n",
    "        \n",
    "        Parameters:\n",
    "        inputDFs (dict): Dictionary of input DataFrames. Keys expected are:\n",
    "                         - \"store\": DataFrame containing store data.\n",
    "                         - \"address\": DataFrame containing address data.\n",
    "                         - \"city\": DataFrame containing city data.\n",
    "                         - \"customer\": DataFrame containing customer data.\n",
    "                         - \"payment\": DataFrame containing payment data.\n",
    "                         \n",
    "        Returns:\n",
    "        dict: A dictionary containing two DataFrames:\n",
    "              - \"sales_by_district\": DataFrame with total sales aggregated by district.\n",
    "              - \"sales_by_city\": DataFrame with total sales aggregated by city.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            \n",
    "            # Ensure all values in inputDFs are Spark DataFrames\n",
    "            for df in inputDFs.values():\n",
    "                assert isinstance(df, DataFrame), \"Non-DataFrame value found in input dictionary.\"\n",
    "        \n",
    "            # Load the input DataFrames from the inputDFs dictionary\n",
    "            store_df = inputDFs[\"store\"]\n",
    "            address_df = inputDFs[\"address\"]\n",
    "            city_df = inputDFs[\"city\"]\n",
    "            customer_df = inputDFs[\"customer\"]\n",
    "            payment_df = inputDFs[\"payment\"]\n",
    "\n",
    "            # Join store, address, city, customer, and payment DataFrames\n",
    "            # using their corresponding keys, and filter for only active customers\n",
    "            joined_df1 = (store_df.join(address_df, \"address_id\")  # Join store with address on address_id\n",
    "                              .join(city_df, \"city_id\")        # Join result with city on city_id\n",
    "                              .join(customer_df, \"store_id\")   # Join result with customer on store_id\n",
    "                              .join(payment_df, \"customer_id\") # Join result with payment on customer_id\n",
    "                              .filter(sf.col(\"active\") == 1))  # Filter for only active customers\n",
    "        \n",
    "            # Calculate total sales by district\n",
    "            sales_by_district_df = (joined_df1.select(\"district\", \"amount\")  # Select district and amount columns\n",
    "                                        .groupBy(\"district\")            # Group by district\n",
    "                                        .agg((sf.round(sf.sum(\"amount\"), 2).alias(\"total_sales\")))  # Sum amount and round\n",
    "                                        .orderBy(\"total_sales\", ascending=False))  # Order by total sales in descending order\n",
    "        \n",
    "            # Calculate total sales by city\n",
    "            sales_by_city_df = (joined_df1.select(\"city\", \"amount\")          # Select city and amount columns\n",
    "                                     .groupBy(\"city\")                    # Group by city\n",
    "                                     .agg((sf.round(sf.sum(\"amount\"), 2)).alias(\"total_sales\"))  # Sum amount and round\n",
    "                                     .orderBy(\"total_sales\", ascending=False))  # Order by total sales in descending order\n",
    "\n",
    "            # Return both aggregated DataFrames in a dictionary\n",
    "            return {\"sales_by_district\": sales_by_district_df,\n",
    "                \"sales_by_city\": sales_by_city_df}\n",
    "            \n",
    "        except AssertionError as e:\n",
    "            print(\"Error: All values in the input dictionary must be Spark DataFrames.\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2182b9d-7e68-4cf6-8bef-e2784f2f992f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actor\naddress\ncategory\ncity\ncountry\ncustomer\nfilm\nfilm_actor\nfilm_category\ninventory\nlanguage\npayment\npayment_p2022_01\npayment_p2022_02\npayment_p2022_03\npayment_p2022_04\npayment_p2022_05\npayment_p2022_06\npayment_p2022_07\nrental\nstaff\nstore\n"
     ]
    }
   ],
   "source": [
    "#testing the transformations\n",
    "# DFs = TablesExtractor().extract()\n",
    "\n",
    "# for key,_ in DFs.items():\n",
    "#     print(key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e7e9dd2-e3e9-4889-a557-64bf652b42da",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# DFs[\"film\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d746e04-03fb-466c-9ee0-58900a1eba62",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# tran = PaymentsTableUnionTransform()\n",
    "# transformedDFs = tran.transform(DFs)\n",
    "\n",
    "# for key,_ in transformedDFs.items():\n",
    "#     print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "353eb392-9e31-4181-8b01-05d418c9a9c8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# display(DFs[\"customer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0145bff7-56bb-44ee-8455-c8cc1fbd446b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Payment DF Schema:\nroot\n |-- payment_id: integer (nullable = true)\n |-- customer_id: integer (nullable = true)\n |-- staff_id: integer (nullable = true)\n |-- rental_id: integer (nullable = true)\n |-- amount: decimal(38,18) (nullable = true)\n |-- payment_date: timestamp (nullable = true)\n\nRental DF Schema:\nroot\n |-- rental_id: integer (nullable = true)\n |-- rental_date: timestamp (nullable = true)\n |-- inventory_id: integer (nullable = true)\n |-- customer_id: integer (nullable = true)\n |-- return_date: timestamp (nullable = true)\n |-- staff_id: integer (nullable = true)\n |-- last_update: timestamp (nullable = true)\n\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'top_customers_by_sales_df': DataFrame[customer_id: int, first_name: string, last_name: string, Total_Spent: double],\n",
       " 'top_customers_by_frequency_df': DataFrame[customer_id: int, first_name: string, last_name: string, Total_Frequency: bigint]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top10 = Top10PercentCustomerTransformation()\n",
    "# top10.transform(DFs)\n",
    "\n",
    "# transformer = CustTableTransformations()\n",
    "# transformer.transform(DFs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70382fbe-fe19-49dc-8410-e583eb37c780",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# transformer2 = SalesRevenueByGeographyTransformation()\n",
    "# transformer2.transform(DFs)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Transforms",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
