{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "436413ed-9619-45df-9bbf-2737abf42203",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class DataSource:\n",
    "    \"\"\"\n",
    "    Abstract base class for data sources. This class defines the structure \n",
    "    for any data source from which a DataFrame will be retrieved. \n",
    "    Subclasses must implement the `get_data_frame` method.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path: str):\n",
    "        \"\"\"\n",
    "        Initializes the DataSource class with the required attributes.\n",
    "        \n",
    "        Parameters:\n",
    "        path (str): The file path or location from which the data will be sourced.\n",
    "        \"\"\"\n",
    "        self.path = path  # The path or location of the data source\n",
    "    \n",
    "    def get_data_frame(self) -> None:\n",
    "        \"\"\"\n",
    "        Abstract method that should be implemented by subclasses. \n",
    "        This method will handle the logic for retrieving a DataFrame from the source.\n",
    "        \n",
    "        Raises:\n",
    "        ValueError: If the method is not defined in a subclass.\n",
    "        \"\"\"\n",
    "        raise ValueError(\"Method not defined\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2178f472-37fb-45bc-9262-045f948026cd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "\n",
    "class ParquetDataSource(DataSource):\n",
    "    \"\"\"\n",
    "    A concrete implementation of the DataSource class for reading data from a CSV file.\n",
    "    This class reads the CSV file from the provided path and loads it as a Spark DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    def get_data_frame(self) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Reads the CSV file from the specified path and returns it as a Spark DataFrame.\n",
    "        This method uses the 'csv' format with the 'header' and 'inferSchema' options enabled.\n",
    "        \n",
    "        Returns:\n",
    "        DataFrame: The loaded Spark DataFrame containing the data from the CSV file.\n",
    "        \"\"\"\n",
    "        # Use Spark to read the CSV file with header and schema inference enabled\n",
    "        return (spark.read\n",
    "                    .format(\"csv\")\n",
    "                    .format(\"parquet\")\n",
    "                    .load(self.path))           # Load the parquet file from the given path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f36f03d-7c18-400d-8058-533c3915fd38",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class CSVDataSource(DataSource):\n",
    "    \"\"\"\n",
    "    A concrete implementation of the DataSource class for reading data from a CSV file.\n",
    "    This class reads the CSV file from the provided path and loads it as a Spark DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    def get_data_frame(self) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Reads the CSV file from the specified path and returns it as a Spark DataFrame.\n",
    "        This method uses the 'csv' format with the 'header' and 'inferSchema' options enabled.\n",
    "        \n",
    "        Returns:\n",
    "        DataFrame: The loaded Spark DataFrame containing the data from the CSV file.\n",
    "        \"\"\"\n",
    "        # Use Spark to read the CSV file with header and schema inference enabled\n",
    "        return (spark.read\n",
    "                    .format(\"csv\")\n",
    "                    .option(\"header\", True)      # First row contains column headers\n",
    "                    .option(\"inferSchema\", True)  # Automatically infer the schema\n",
    "                    .load(self.path)  )            # Load the CSV file from the given path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa781b0e-4010-4a9d-a5ab-9ae56f2424ba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class DeltaDataSource(DataSource):\n",
    "    \"\"\"\n",
    "    A concrete implementation of the DataSource class for reading data from a Delta table.\n",
    "    This class reads a Delta table based on the provided table name and loads it as a Spark DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    def get_data_frame(self) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Reads the Delta table specified by the path (interpreted as the table name)\n",
    "        and returns it as a Spark DataFrame.\n",
    "        \n",
    "        Returns:\n",
    "        DataFrame: The loaded Spark DataFrame containing the data from the Delta table.\n",
    "        \"\"\"\n",
    "        # Interpret the path as the Delta table name\n",
    "        table_name: str = self.path\n",
    "        \n",
    "        # Use Spark to read the Delta table and return it as a DataFrame\n",
    "        return spark.read.table(table_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bb7fbb3-fa37-46b8-8ea8-1ee0ec99be1d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "def get_data(file_type: str, file_path: str) -> Union[CSVDataSource, ParquetDataSource, DeltaDataSource]:\n",
    "    \"\"\"\n",
    "    Factory function that returns the appropriate DataSource object based on the file type.\n",
    "    \n",
    "    Parameters:\n",
    "    file_type (str): The type of file to read. Valid options are 'csv', 'parquet', or 'delta'.\n",
    "    file_path (str): The file path or table name from which the data will be sourced.\n",
    "    \n",
    "    Returns:\n",
    "    DataSource: An instance of a DataSource subclass (CSVDataSource, ParquetDataSource, or DeltaDataSource).\n",
    "    \n",
    "    Raises:\n",
    "    ValueError: If the provided file_type is not supported.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check the file type and return the corresponding DataSource class\n",
    "    if file_type.lower() == \"csv\":\n",
    "        return CSVDataSource(file_path)  # Return a CSVDataSource instance\n",
    "    elif file_type.lower() == \"parquet\":\n",
    "        return ParquetDataSource(file_path)  # Return a ParquetDataSource instance\n",
    "    elif file_type.lower() == \"delta\":\n",
    "        return DeltaDataSource(file_path)  # Return a DeltaDataSource instance\n",
    "    else:\n",
    "        # Raise an error for unsupported file types\n",
    "        raise ValueError(f\"Method not implemented for datatype: {file_type}\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Extractor_Factory",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
