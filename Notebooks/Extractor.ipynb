{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0a35b51-a3f5-4d3e-92c4-4e2f55f1acfa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"./Extractor_Factory\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34bb0f52-89c5-4e64-a6d7-aa9e061f6625",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "class Extractor:\n",
    "    \"\"\"\n",
    "    Abstract base class for extractors. Provides a base structure for \n",
    "    extraction logic that can be extended by other classes.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the Extractor base class.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def extract(self):\n",
    "        \"\"\"\n",
    "        Abstract method that should be implemented by any subclass. \n",
    "        Defines the contract for the data extraction process.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class TablesExtractor(Extractor):\n",
    "    \"\"\"\n",
    "    Concrete implementation of the Extractor class for extracting tables. \n",
    "    This class retrieves data from parquet files stored in a specified path \n",
    "    and returns them as a dictionary of DataFrames.\n",
    "    \"\"\"\n",
    "    \n",
    "    def extract(self) -> Dict[str, DataFrame]:\n",
    "        \"\"\"\n",
    "        Extracts data from parquet files located in a given directory structure \n",
    "        and returns the tables as a dictionary of DataFrames. The table names are \n",
    "        used as keys, and the DataFrames are the values.\n",
    "        \n",
    "        Returns:\n",
    "        dict: A dictionary where each key is a table name, and each value is the \n",
    "              corresponding DataFrame loaded from a parquet file.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Dictionary to store the DataFrames with table names as keys\n",
    "        inputDFs = {}\n",
    "        \n",
    "        # Iterate over all the directories in the specified path on DBFS\n",
    "        for schema_info in dbutils.fs.ls(\"dbfs:/mnt/bronze/public/\"):\n",
    "            # Extract the actual path from the FileInfo object\n",
    "            table_path = schema_info.path\n",
    "            table_name = table_path.rstrip('/').split('/')[-1]  # Get the table name from the path\n",
    "    \n",
    "            # Construct the file path for the corresponding parquet file\n",
    "            file_path = f\"{table_path}{table_name}.parquet\"\n",
    "    \n",
    "            # Get the DataFrame by reading the parquet file\n",
    "            df = get_data(file_type=\"parquet\", file_path=file_path).get_data_frame()\n",
    "            \n",
    "            # Store the DataFrame in the dictionary with the table name as the key\n",
    "            inputDFs[table_name] = df \n",
    "\n",
    "        # Return the dictionary of DataFrames\n",
    "        return inputDFs\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Extractor",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
